{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Column 22 (Rpt_Block_Num) is almost all int except for when they put 111 1/2 or 111 BLK\n",
    "# rpt_block_num_cnvtr = lambda x: x.split(' ')[0] if len(x.split(' ')) > 1 and x.split(' ')[1] in ['1/2', 'BLK'] else x\n",
    "# rpt_sec_hwy_sfx_dict = {'W': 22, 'R': 17, 'E': 5, 'S': 18, 'N': 14, 'M': 13, 'A': 1}\n",
    "# rpt_sec_hwy_sfx_cnvtr = lambda x: rpt_sec_hwy_sfx_dict[x] if type(x) == str and len(x) > 0 else x\n",
    "# crash_10_cnvtr = {22: rpt_block_num_cnvtr}\n",
    "# crash_12_cnvtr = {22: rpt_block_num_cnvtr, 35: rpt_sec_hwy_sfx_cnvtr, 20: rpt_sec_hwy_sfx_cnvtr}\n",
    "# dtypes = {'Rpt_Sec_Hwy_Num': str, 'Rpt_Sec_Block_Num': str, 'Street_Nbr': str, \\\n",
    "#           'Rpt_CrossingNumber': str, 'Rpt_CrossingNumber': str, 'CrossingNumber': str, 'RRCo': str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# crash_10 skipped rows have data split incorrectly in csv file starting at col 33\n",
    "# crash_15, 16, 17 skipped rows have Rpt_Hwy_Sfx values entered incorrectly \n",
    "#     (entered actual sfx rather than data dict code)\n",
    "crash_10 = pd.read_csv('data/2010/extract_public_2010_20170913160331_crash_20100101-20101231_TRAVIS.csv', \\\n",
    "                       converters=crash_10_cnvtr, skiprows=[2495, 3258, 3580, 8252, 9523], dtype=dtypes, \\\n",
    "                       index_col='Crash_ID')\n",
    "crash_11 = pd.read_csv('data/2011/extract_public_2010_20170913151637_crash_20110101-20111231_TRAVIS.csv', \\\n",
    "                      converters=crash_10_cnvtr, dtype=dtypes, index_col='Crash_ID')\n",
    "crash_12 = pd.read_csv('data/2012/extract_public_2010_20170913151619_crash_20120101-20121231_TRAVIS.csv', \\\n",
    "                      converters=crash_12_cnvtr, dtype=dtypes, index_col='Crash_ID')\n",
    "crash_13 = pd.read_csv('data/2013/extract_public_2010_20170913151600_crash_20130101-20131231_TRAVIS.csv', \\\n",
    "                      dtype=dtypes, index_col='Crash_ID')\n",
    "crash_14 = pd.read_csv('data/2014/extract_public_2010_20170913151542_crash_20140101-20141231_TRAVIS.csv', \\\n",
    "                       dtype=dtypes, index_col='Crash_ID')\n",
    "crash_15 = pd.read_csv('data/2015/extract_public_2015_20170829153840_crash_20150101-20151231_TRAVIS.csv', \\\n",
    "                       converters=crash_12_cnvtr, dtype=dtypes, index_col='Crash_ID', \\\n",
    "                       skiprows=[5177, 5446, 6653, 9839, 9841, 14381, 19176])\n",
    "crash_16 = pd.read_csv('data/2016/extract_public_2015_20170829153815_crash_20160101-20161231_TRAVIS.csv', \\\n",
    "                       converters=crash_12_cnvtr, dtype=dtypes, skiprows=[2432, 3794, 3795, 8077, 8188, 20173], \\\n",
    "                       index_col='Crash_ID')\n",
    "crash_17 = pd.read_csv('data/2017/extract_public_2015_20170829132341_crash_20170101-20170829_TRAVIS.csv', \\\n",
    "                      converters=crash_12_cnvtr, dtype=dtypes, skiprows=[4480], index_col='Crash_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crash = pd.concat([crash_10, crash_11, crash_12, crash_13, crash_14, crash_15, crash_16, crash_17])\n",
    "# crash['Crash_Timestamp'] = crash.loc[:,'Crash_Date'] + ' ' + crash.loc[:,'Crash_Time']\n",
    "# crash['Crash_Timestamp'] = pd.to_datetime(crash['Crash_Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coord_lat = crash['Latitude'].fillna(0).values\n",
    "# coord_long = crash['Longitude'].fillna(0).values\n",
    "# coord = set(zip(coord_lat, coord_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cPickle as pickle\n",
    "\n",
    "# with open('coordinates1.txt', 'r') as f:\n",
    "#     locations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from geopy.geocoders import GoogleV3\n",
    "# from geopy.exc import GeocoderTimedOut\n",
    "# import time\n",
    "# geolocator = GoogleV3(api_key='AIzaSyCBDrJBXXY8Hp-mkDwHCJm_eASiWmS6bhg')\n",
    "# for i, pair in enumerate(coord):\n",
    "#     if pair in locations:\n",
    "#         continue\n",
    "#     else:\n",
    "#         try:\n",
    "#             locations[pair] = geolocator.reverse(pair, timeout=None, exactly_one=True)\n",
    "#         except GeocoderTimedOut as e:\n",
    "#             time.sleep(3)\n",
    "#             locations[pair] = geolocator.reverse(pair, timeout=None, exactly_one=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cPickle as pickle\n",
    "\n",
    "# with open('coordinates1.txt', 'w') as f:\n",
    "#     f.write(pickle.dumps(locations, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# zip_codes = {}\n",
    "# for key, value in locations.iteritems():\n",
    "#     if value:\n",
    "#         if re.search('TX (\\d*)', value[0]):\n",
    "#             zip_codes[key] = re.search('TX (\\d*)', value[0]).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crash['Crash_Zip'] = crash.loc[:,['Latitude','Longitude']].apply(lambda x: zip_codes[(x['Latitude'], x['Longitude'])] \\\n",
    "#                                             if (x['Latitude'], x['Longitude']) in zip_codes else 'Unkwn', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking no new duplicates from concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crash[crash.Crash_ID.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Damages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "damages_10 = pd.read_csv('data/2010/extract_public_2010_20170913160331_damages_20100101-20101231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "damages_11 = pd.read_csv('data/2011/extract_public_2010_20170913151637_damages_20110101-20111231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "damages_12 = pd.read_csv('data/2012/extract_public_2010_20170913151619_damages_20120101-20121231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "damages_13 = pd.read_csv('data/2013/extract_public_2010_20170913151600_damages_20130101-20131231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "damages_14 = pd.read_csv('data/2014/extract_public_2010_20170913151542_damages_20140101-20141231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "damages_15 = pd.read_csv('data/2015/extract_public_2015_20170829153840_damages_20150101-20151231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "damages_16 = pd.read_csv('data/2016/extract_public_2015_20170829153815_damages_20160101-20161231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "damages_17 = pd.read_csv('data/2017/extract_public_2015_20170829132341_damages_20170101-20170829_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "damages = pd.concat([damages_10, damages_11, damages_12, damages_13, damages_14, damages_15, damages_16, damages_17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking no new duplicates from concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# damages['dup'] = damages['Crash_ID'].map(str) + damages['Damaged_Property'].map(str)\n",
    "# damages = damages.sort_values(by='dup')\n",
    "# duplicates shown exist in csv. must be 'multiple counts of damages' or something like that\n",
    "# damages[damages['dup'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cmv_carrier_zip_cnvtr = lambda x: x.replace('-', '') if '-' in str(x) else x\n",
    "# unit_10_cnvtr = {31: cmv_carrier_zip_cnvtr}\n",
    "# dtypes = {80: str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Unit_16 skipped rows had column values shifted in the csv\n",
    "unit_10 = pd.read_csv('data/2010/extract_public_2010_20170913160331_unit_20100101-20101231_TRAVIS.csv', \\\n",
    "                     converters=unit_10_cnvtr, index_col='Crash_ID')\n",
    "unit_11 = pd.read_csv('data/2011/extract_public_2010_20170913151637_unit_20110101-20111231_TRAVIS.csv', \\\n",
    "                      index_col='Crash_ID')\n",
    "unit_12 = pd.read_csv('data/2012/extract_public_2010_20170913151619_unit_20120101-20121231_TRAVIS.csv', \\\n",
    "                      index_col='Crash_ID')\n",
    "unit_13 = pd.read_csv('data/2013/extract_public_2010_20170913151600_unit_20130101-20131231_TRAVIS.csv', \\\n",
    "                      index_col='Crash_ID')\n",
    "unit_14 = pd.read_csv('data/2014/extract_public_2010_20170913151542_unit_20140101-20141231_TRAVIS.csv', \\\n",
    "                      index_col='Crash_ID')\n",
    "unit_15 = pd.read_csv('data/2015/extract_public_2015_20170829153840_unit_20150101-20151231_TRAVIS.csv', \\\n",
    "                      index_col='Crash_ID')\n",
    "unit_16 = pd.read_csv('data/2016/extract_public_2015_20170829153815_unit_20160101-20161231_TRAVIS.csv', \\\n",
    "                     skiprows=[43205], dtype=dtypes, index_col='Crash_ID')\n",
    "unit_17 = pd.read_csv('data/2017/extract_public_2015_20170829132341_unit_20170101-20170829_TRAVIS.csv', \\\n",
    "                      index_col='Crash_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit = pd.concat([unit_10, unit_11, unit_12, unit_13, unit_14, unit_15, unit_16, unit_17])\n",
    "unit = unit.drop(['Incap_Injry_Cnt', 'Nonincap_Injry_Cnt', 'Poss_Injry_Cnt', 'Non_Injry_Cnt', \\\n",
    "                        'Unkn_Injry_Cnt', 'Tot_Injry_Cnt', 'Death_Cnt'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking no new duplicates from concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unit['dup'] = unit['Crash_ID'].map(str) + unit['Unit_Nbr'].map(str)\n",
    "# unit = unit.sort_values(by='dup')\n",
    "# unit[unit['dup'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PrimaryPerson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primaryperson_10 = pd.read_csv('data/2010/extract_public_2010_20170913160331_primaryperson_20100101-20101231_TRAVIS.csv', \\\n",
    "                               index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "primaryperson_11 = pd.read_csv('data/2011/extract_public_2010_20170913151637_primaryperson_20110101-20111231_TRAVIS.csv', \\\n",
    "                               index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "primaryperson_12 = pd.read_csv('data/2012/extract_public_2010_20170913151619_primaryperson_20120101-20121231_TRAVIS.csv', \\\n",
    "                               index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "primaryperson_13 = pd.read_csv('data/2013/extract_public_2010_20170913151600_primaryperson_20130101-20131231_TRAVIS.csv', \\\n",
    "                               index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "primaryperson_14 = pd.read_csv('data/2014/extract_public_2010_20170913151542_primaryperson_20140101-20141231_TRAVIS.csv', \\\n",
    "                               index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "primaryperson_15 = pd.read_csv('data/2015/extract_public_2015_20170829153840_primaryperson_20150101-20151231_TRAVIS.csv', \\\n",
    "                               index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "primaryperson_16 = pd.read_csv('data/2016/extract_public_2015_20170829153815_primaryperson_20160101-20161231_TRAVIS.csv', \\\n",
    "                               index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "primaryperson_17 = pd.read_csv('data/2017/extract_public_2015_20170829132341_primaryperson_20170101-20170829_TRAVIS.csv', \\\n",
    "                               index_col=['Crash_ID', 'Unit_Nbr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primaryperson = pd.concat([primaryperson_10, primaryperson_11, primaryperson_12, primaryperson_13, primaryperson_14, primaryperson_15, primaryperson_16, primaryperson_17])\n",
    "primaryperson = primaryperson.drop(['Incap_Injry_Cnt', 'Nonincap_Injry_Cnt', 'Poss_Injry_Cnt', 'Non_Injry_Cnt', \\\n",
    "                        'Unkn_Injry_Cnt', 'Tot_Injry_Cnt', 'Death_Cnt'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking no new duplicates from concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# primaryperson['dup'] = primaryperson['Crash_ID'].map(str) + primaryperson['Unit_Nbr'].map(str) + primaryperson['Prsn_Nbr'].map(str)\n",
    "# primaryperson = primaryperson.sort_values(by='dup')\n",
    "# primaryperson[primaryperson['dup'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person_10 = pd.read_csv('data/2010/extract_public_2010_20170913160331_person_20100101-20101231_TRAVIS.csv', \\\n",
    "                        index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "person_11 = pd.read_csv('data/2011/extract_public_2010_20170913151637_person_20110101-20111231_TRAVIS.csv', \\\n",
    "                        index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "person_12 = pd.read_csv('data/2012/extract_public_2010_20170913151619_person_20120101-20121231_TRAVIS.csv', \\\n",
    "                        index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "person_13 = pd.read_csv('data/2013/extract_public_2010_20170913151600_person_20130101-20131231_TRAVIS.csv', \\\n",
    "                        index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "person_14 = pd.read_csv('data/2014/extract_public_2010_20170913151542_person_20140101-20141231_TRAVIS.csv', \\\n",
    "                        index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "person_15 = pd.read_csv('data/2015/extract_public_2015_20170829153840_person_20150101-20151231_TRAVIS.csv', \\\n",
    "                        index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "person_16 = pd.read_csv('data/2016/extract_public_2015_20170829153815_person_20160101-20161231_TRAVIS.csv', \\\n",
    "                        index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "person_17 = pd.read_csv('data/2017/extract_public_2015_20170829132341_person_20170101-20170829_TRAVIS.csv', \\\n",
    "                        index_col=['Crash_ID', 'Unit_Nbr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person = pd.concat([person_10, person_11, person_12, person_13, person_14, person_15, person_16, person_17])\n",
    "person = person.drop(['Incap_Injry_Cnt', 'Nonincap_Injry_Cnt', 'Poss_Injry_Cnt', 'Non_Injry_Cnt', \\\n",
    "                        'Unkn_Injry_Cnt', 'Tot_Injry_Cnt', 'Death_Cnt'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking no new duplicates from concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# person['dup'] = person['Crash_ID'].map(str) + person['Unit_Nbr'].map(str) + person['Prsn_Nbr'].map(str)\n",
    "# person = person.sort_values(by='dup')\n",
    "# person[person['dup'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Endorsements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endorsements_10 = pd.read_csv('data/2010/extract_public_2010_20170913160331_endorsements_20100101-20101231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "endorsements_11 = pd.read_csv('data/2011/extract_public_2010_20170913151637_endorsements_20110101-20111231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "endorsements_12 = pd.read_csv('data/2012/extract_public_2010_20170913151619_endorsements_20120101-20121231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "endorsements_13 = pd.read_csv('data/2013/extract_public_2010_20170913151600_endorsements_20130101-20131231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "endorsements_14 = pd.read_csv('data/2014/extract_public_2010_20170913151542_endorsements_20140101-20141231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "endorsements_15 = pd.read_csv('data/2015/extract_public_2015_20170829153840_endorsements_20150101-20151231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "endorsements_16 = pd.read_csv('data/2016/extract_public_2015_20170829153815_endorsements_20160101-20161231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "endorsements_17 = pd.read_csv('data/2017/extract_public_2015_20170829132341_endorsements_20170101-20170829_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endorsements = pd.concat([endorsements_10, endorsements_11, endorsements_12, endorsements_13, endorsements_14, endorsements_15, endorsements_16, endorsements_17])\n",
    "endorsements['Prsn_Nbr'] = 1.0\n",
    "endorsements = endorsements.reset_index().set_index(['Crash_ID', 'Unit_Nbr', 'Prsn_Nbr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking no new duplicates from concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# endorsements['dup'] = endorsements['Crash_ID'].map(str) + endorsements['Unit_Nbr'].map(str) + endorsements['Drvr_Lic_Endors_ID'].map(str)\n",
    "# endorsements = endorsements.sort_values(by='dup')\n",
    "# endorsements[endorsements['dup'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restrictions_10 = pd.read_csv('data/2010/extract_public_2010_20170913160331_restrictions_20100101-20101231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "restrictions_11 = pd.read_csv('data/2011/extract_public_2010_20170913151637_restrictions_20110101-20111231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "restrictions_12 = pd.read_csv('data/2012/extract_public_2010_20170913151619_restrictions_20120101-20121231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "restrictions_13 = pd.read_csv('data/2013/extract_public_2010_20170913151600_restrictions_20130101-20131231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "restrictions_14 = pd.read_csv('data/2014/extract_public_2010_20170913151542_restrictions_20140101-20141231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "restrictions_15 = pd.read_csv('data/2015/extract_public_2015_20170829153840_restrictions_20150101-20151231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "restrictions_16 = pd.read_csv('data/2016/extract_public_2015_20170829153815_restrictions_20160101-20161231_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])\n",
    "restrictions_17 = pd.read_csv('data/2017/extract_public_2015_20170829132341_restrictions_20170101-20170829_TRAVIS.csv', \\\n",
    "                              index_col=['Crash_ID', 'Unit_Nbr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restrictions = pd.concat([restrictions_10, restrictions_11, restrictions_12, restrictions_13, restrictions_14, restrictions_15, restrictions_16, restrictions_17])\n",
    "restrictions['Prsn_Nbr'] = 1.0\n",
    "restrictions = restrictions.reset_index().set_index(['Crash_ID', 'Unit_Nbr', 'Prsn_Nbr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking no new duplicates from concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# restrictions['dup'] = restrictions['Crash_ID'].map(str) + restrictions['Unit_Nbr'].map(str) + restrictions['Drvr_Lic_Restric_ID'].map(str)\n",
    "# restrictions = restrictions.sort_values(by='dup')\n",
    "# restrictions[restrictions['dup'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "charges_10 = pd.read_csv('data/2010/extract_public_2010_20170913160331_charges_20100101-20101231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "charges_11 = pd.read_csv('data/2011/extract_public_2010_20170913151637_charges_20110101-20111231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "charges_12 = pd.read_csv('data/2012/extract_public_2010_20170913151619_charges_20120101-20121231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "charges_13 = pd.read_csv('data/2013/extract_public_2010_20170913151600_charges_20130101-20131231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "charges_14 = pd.read_csv('data/2014/extract_public_2010_20170913151542_charges_20140101-20141231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "charges_15 = pd.read_csv('data/2015/extract_public_2015_20170829153840_charges_20150101-20151231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "charges_16 = pd.read_csv('data/2016/extract_public_2015_20170829153815_charges_20160101-20161231_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')\n",
    "charges_17 = pd.read_csv('data/2017/extract_public_2015_20170829132341_charges_20170101-20170829_TRAVIS.csv', \\\n",
    "                         index_col='Crash_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "charges = pd.concat([charges_10, charges_11, charges_12, charges_13, charges_14, charges_15, charges_16, charges_17])\n",
    "charges = charges.reset_index().set_index(['Crash_ID', 'Unit_Nbr', 'Prsn_Nbr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking no new duplicates from concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# charges['dup'] = charges.Crash_ID.map(str) + charges.Unit_Nbr.map(str) + charges.Prsn_Nbr.map(str) + charges.Charge_Cat_ID.map(str) + charges.Charge + charges.Citation_Nbr.map(str)\n",
    "# charges = charges.sort_values(by='dup')\n",
    "# charges[charges.dup.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining each section of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined = crash.join(damages, how='left')\n",
    "joined = joined.join(unit, how='left')\n",
    "joined['Num_Cars'] = joined.Unit_Nbr.max(level=0)\n",
    "joined = joined.reset_index().set_index(['Crash_ID', 'Unit_Nbr'])\n",
    "\n",
    "people = pd.concat([person, primaryperson])\n",
    "people = people.sort_index()\n",
    "joined = joined.join(people, how='left')\n",
    "ppl_total = joined.Prsn_Nbr.count(level=0)\n",
    "joined = joined.reset_index().set_index('Crash_ID')\n",
    "joined['Num_Ppl'] = ppl_total\n",
    "joined = joined.reset_index().set_index(['Crash_ID', 'Unit_Nbr'])\n",
    "# joined = joined.reset_index().set_index(['Crash_ID', 'Unit_Nbr', 'Prsn_Nbr'])\n",
    "# joined = joined.sort_index()\n",
    "# joined = joined.join(endorsements, how='left')\n",
    "# joined = joined.join(restrictions, how='left')\n",
    "# joined = joined.join(charges, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = joined.loc[:,['Crash_Timestamp', 'Street_Name', 'Crash_Speed_Limit', \\\n",
    "                         'Veh_Mod_Year', 'Wthr_Cond_ID', 'Prsn_Injry_Sev_ID', 'Num_Cars', 'Num_Ppl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features['years ago'] = 2017 - features.loc[:, 'Crash_Timestamp'].dt.year.values\n",
    "features['month'] = features.loc[:, 'Crash_Timestamp'].dt.month.values\n",
    "features['weekday'] = features.loc[:,'Crash_Timestamp'].dt.weekday.values\n",
    "features['hour'] = features.loc[:, 'Crash_Timestamp'].dt.hour.values\n",
    "features = features.drop('Crash_Timestamp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "injry_sev_map = {0: 'UNKNOWN', 1: 'INJURY', 2: 'INJURY', 3: 'NOT INJURED', \\\n",
    "                 4: 'INJURY', 5: 'NOT INJURED', 94: 'REPORTED INVALID', 95: 'NOT REPORTED'}\n",
    "\n",
    "features['Prsn_Injry_Sev_ID'] = features['Prsn_Injry_Sev_ID'].map(injry_sev_map)\n",
    "features.drop(features[features['Prsn_Injry_Sev_ID'] == 'UNKNOWN'].index, inplace=True)\n",
    "features.drop(features[features['Prsn_Injry_Sev_ID'] == 'REPORTED INVALID'].index, inplace=True)\n",
    "features.drop(features[features['Prsn_Injry_Sev_ID'] == 'NOT REPORTED'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wthr_cond_map = {0: 'UNKNOWN', 2: 'SEVERE', 3: 'SEVERE', 4: 'SEVERE', 5: 'SEVERE', 6: 'SEVERE', \\\n",
    "                7: 'SEVERE', 8: 'UNKNOWN', 11: 'FINE', 12: 'FINE', 94: 'UNKNOWN', 95: 'UNKNOWN'}\n",
    "features['Wthr_Cond_ID'] = features['Wthr_Cond_ID'].map(wthr_cond_map)\n",
    "features.drop(features[features['Wthr_Cond_ID'] == 'UNKNOWN'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "street_name_set = {'IH0035', 'US0183', 'US0290', 'SL0001', 'SH0071', 'RM0620', 'SL0360', 'SL0275', 'SL0343', 'FM0734'}\n",
    "features['Street_Name'] = features['Street_Name'].map(lambda x: x if x in street_name_set else 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features.drop(features[features['Crash_Speed_Limit'] < 1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = pd.get_dummies(features, columns=['Wthr_Cond_ID', 'Street_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_map = {'INJURY': 1, 'NOT INJURED': 0}\n",
    "features['label'] = features['Prsn_Injry_Sev_ID'].map(label_map)\n",
    "features = features.drop('Prsn_Injry_Sev_ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275373, 22)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y = features['label']\n",
    "X = features.drop(['label'], axis=1)\n",
    "oversampler = SMOTE(ratio='minority', random_state=1)\n",
    "X_resampled, y_resampled = oversampler.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 243462, 1: 243462})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
       "       max_depth=11, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.9)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rf = RandomForestClassifier(max_depth=None, max_features=3, n_estimators=50, random_state=1)\n",
    "# rf.fit(X_train, y_train)\n",
    "# rf_params = {'n_estimators': [10, 25, 50], 'max_features': [3, 9, 'auto', None], 'max_depth': [None, 1, 5], \\\n",
    "#              'random_state': [1]}\n",
    "\n",
    "# ada = AdaBoostClassifier()\n",
    "# ada.fit(X_train, y_train)\n",
    "# ada_params = {'n_estimators': [10, 25, 50], 'learning_rate': [.75, .85, .95, 1], 'random_state': [1]}\n",
    "\n",
    "xgb = XGBClassifier(learning_rate=.3, max_depth=11, subsample=.9)\n",
    "xgb.fit(X_train, y_train)\n",
    "# xgb_params = {'max_depth': [8, 9, 10, 11], 'learning_rate': [.25, .3, .35, .4], 'n_estimators': [150], \\\n",
    "#               'subsample': [.8, .9, 1]}\n",
    "# grid = GridSearchCV(estimator=xgb, param_grid=xgb_params, n_jobs=2, verbose=3, scoring='f1')\n",
    "# grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grid.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91642820958665916"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rf.score(X_test, y_test)\n",
    "# zip(f1_score(rf.predict(X_test), y_test, average=None), rf.classes_)\n",
    "# zip(f1_score(ada.predict(X_test), y_test, average=None), ada.classes_)\n",
    "# zip(f1_score(xgb.predict(X_test), y_test, average=None), xgb.classes_)\n",
    "xgb.score(X_test, y_test)\n",
    "# zip(f1_score(grid.best_estimator_.predict(X_test), y_test, average=None), grid.best_estimator_.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.92227768157278189, 0), (0.90962660270646412, 1)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rf.score(X_test, y_test)\n",
    "# zip(f1_score(rf.predict(X_test), y_test, average=None), rf.classes_)\n",
    "# zip(f1_score(ada.predict(X_test), y_test, average=None), ada.classes_)\n",
    "zip(f1_score(xgb.predict(X_test), y_test, average=None), xgb.classes_)\n",
    "# zip(f1_score(grid.best_estimator_.predict(X_test), y_test, average=None), grid.best_estimator_.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.93888094531523991, 0), (0.93178670360110805, 1)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rf.score(X_test, y_test)\n",
    "# zip(f1_score(rf.predict(X_test), y_test, average=None), rf.classes_)\n",
    "zip(f1_score(grid.best_estimator_.predict(X_test), y_test, average=None), grid.best_estimator_.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17.284286, 'Prsn_Age'),\n",
       " (8.2165413, 'Crash_Speed_Limit'),\n",
       " (4.0743852, 'Num_Cars'),\n",
       " (8.5960579, 'Num_Ppl'),\n",
       " (13.930983, 'Veh_Age'),\n",
       " (8.6719618, 'years ago'),\n",
       " (10.415029, 'month'),\n",
       " (8.0755777, 'weekday'),\n",
       " (13.410501, 'hour'),\n",
       " (1.0138524, 'Wthr_Cond_ID_FINE'),\n",
       " (0.0, 'Wthr_Cond_ID_SEVERE'),\n",
       " (0.23855351, 'Street_Name_FM0734'),\n",
       " (1.1466833, 'Street_Name_IH0035'),\n",
       " (1.762043, 'Street_Name_Other'),\n",
       " (0.29277021, 'Street_Name_RM0620'),\n",
       " (0.5150587, 'Street_Name_SH0071'),\n",
       " (0.50421536, 'Street_Name_SL0001'),\n",
       " (0.23313183, 'Street_Name_SL0275'),\n",
       " (0.18704763, 'Street_Name_SL0343'),\n",
       " (0.22228849, 'Street_Name_SL0360'),\n",
       " (0.62349212, 'Street_Name_US0183'),\n",
       " (0.58554041, 'Street_Name_US0290')]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(xgb.feature_importances_*100, features.drop('label', axis=1).columns)\n",
    "# zip(grid.best_estimator_.feature_importances_*100, features.drop('label', axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "\n",
    "with open('raw_model_data.pkl', 'w') as f:\n",
    "    f.write(pickle.dumps(joined, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
